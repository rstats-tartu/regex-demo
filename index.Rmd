---
title: "Regexes"
author: "Taavi Päll"
date: "`r lubridate::today()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Find character strings

- Character strings can be matched and manipulated in base R by using regular expressions in functions grep, grepl, sub, gsub, regexpr + regmatches and some others. 

- tidyverse package ‘stringr’ contains analogous verbs with more consistent syntax. 

- A regular expression is a pattern that describes a set of strings.

## Regular Expressions as used in R

- Most characters, including all letters and digits, are regular expressions that match themselves. 

- Whereas, e.g. . matches any single character.

- You can refer also to a character class, which is a list of characters enclosed between [ and ], e.g. [[:alnum:]] is same as [A-z0-9].
- Most common character classes:
    - [:alnum:] includes alphanumerics ([:alpha:] and [:digit:]);
    - [:alpha:], includes alphabetic characters ([:upper:] and [:lower:] case);
    - [:punct:] includes punctuation characters ! " # $ % & ’ ( ) * + , - . / : ; < = > ? @ [  ] ^ _ ` ` { | } ~.;
    - [:blank:] includes space and tab; etc.

- The metacharacters in regular expressions are . \ | ( ) [ { ^ $ * + ?, whether these have a special meaning depending on the context.

- When matching any metacharacter as a regular character, precede it with a double backslash \\.

- Repetition quantifiers put after regex specify how many times regex is matched: ?, optional, at most once; *, zero or more times; +, one or more times; {n}, n times; {n,}, n or more times; {n,m}, n to m times.

- The caret ^ and the dollar sign $ are metacharacters that respectively match the empty string at the beginning and end of a line.

## Common operations with regular expressions

- Locate a pattern match (positions)

- Extract a matched pattern

- Identify a match to a pattern

- Replace a matched pattern

## Let's try out
Download test dataset.

> Test dataset contains Supplementary file names and some metadata of gene expresion profiling experiments using high-throughput sequencing: 

https://www.ncbi.nlm.nih.gov/gds?term=%22expression+profiling+by+high+throughput+sequencing%22[DataSet+Type]
```{r}
if(!dir.exists("data")){
  dir.create("data")
}
## manually download suppfilenames_2017-06-19.RData from rstats-tartu/datasets
## alternatively clone this repo 'rstat-tartu/regex-demo'
```

## Load data
```{r}
load("data/suppfilenames_2017-06-19.RData")
```


## Unnest dataset
```{r}
library(tidyverse)
library(stringr)
## Filter out rows with missing file names
suppfilenames <- suppfilenames %>% 
  filter(!map_lgl(SuppFileNames, ~ inherits(., "try-error")))
suppfilenames %>% select(Accession, PDAT, SuppFileNames)
## unnest supplementary file names
supfn <-  suppfilenames %>% unnest(SuppFileNames)
supfn %>% select(Accession, PDAT, SuppFileNames)
```


## Get string length
To get the length of a text string (i.e. the number of characters in the string).

```{r}
str_length("banana")
str_length("")
```


> Length of supplementary file names.

```{r}
supfn <- supfn %>% 
  select(Accession, PDAT, SuppFileNames) %>% 
  mutate(strlen = str_length(SuppFileNames))
supfn
```

Plot sizedistribution of supplementary file names:
```{r}
ggplot(supfn, aes(strlen)) + geom_histogram(bins = 40)
```

Distribution seems skewed, what if we plot log transformed strlen values?
```{r}
ggplot(supfn, aes(log2(strlen))) + geom_histogram(bins = 40)
```


## Let's look at the filenames
```{r}
# Single most common filename: filelist.txt
most_common_filename <- supfn %>% 
  group_by(SuppFileNames) %>% 
  summarise(N = n()) %>% 
  arrange(desc(N))
most_common_filename
```


## String manipulation

Filenames are prepended with GSE id 
```{r}
# Supplemental file names with more than N = 10 occurences
cf <- supfn %>%
  mutate(common_filenames = str_replace(SuppFileNames, "GSE[0-9]+_", ""),
         common_filenames = str_replace(common_filenames, "\\.gz$", ""),
         common_filenames = str_to_lower(common_filenames))
cf
```

```{r}
cfn <- group_by(cf, common_filenames) %>% 
  summarise(N = n()) %>% 
  arrange(desc(N)) %>% 
  filter(N > 10)
cfn
cfp <- ggplot(cfn, aes(common_filenames, N)) +
  geom_point() +
  scale_x_discrete(limits = rev(cfn$common_filenames)) +
  scale_y_log10() +
  coord_flip() + 
  xlab("Common stubs of SuppFileNames\n(>10 occurences) ") +
  ylab("Number of files")

# plot commonfilenames ggplot
cfp

```

## File name length distribution 2

Now we can filter out "filelist.txt" and "RAW.tar" files and replot file name distribution.
```{r}
filter(supfn, !str_detect(SuppFileNames, "filelist|RAW.tar")) %>% 
  ggplot(aes(log2(strlen))) + geom_histogram(bins = 40)
```


## `str_detect()`

```{r}
## str_detect generates logical vector of matches and nonmatches
## match letter b against alphabet and get index of TRUE values 
str_detect(letters, "b") %>% which
```

## Regular expressions can be ugly

We want to filter out some file types.

```{r}
# we are looking only for tabular data. 
out_string1 <- c("filelist|annotation|readme|error|raw.tar|csfasta|bam|sam|bed|[:punct:]hic|hdf5|bismark|map|barcode|peaks")
out_string2 <- c("tar","gtf","(big)?bed(\\.txt|12|graph|pk)?","bw",
                 "wig","hic","gct(x)?","tdf","gff(3)?","pdf","png","zip",
                 "sif","narrowpeak","fa", "r$", "rda(ta)?$")
paste0(out_string2, "(\\.gz|\\.bz2)?$", collapse = "|")
suppfiles_of_interest <- supfn %>%
  filter(!str_detect(tolower(SuppFileNames), out_string1),
         !str_detect(tolower(SuppFileNames), paste0(out_string2, "(\\.gz|\\.bz2)?$", collapse = "|"))) %>%
  mutate(filext = str_extract(str_to_lower(SuppFileNames), "\\.[:alpha:]+([:punct:][bgz2]+)?$"))
suppfiles_of_interest
```

Most popular file extensions of potentially interesting files.
```{r}
fext <- group_by(suppfiles_of_interest, filext) %>% 
  summarise(N = n()) %>% 
  arrange(desc(N)) %>% 
  filter(N > 10)
fext
```

```{r}
ggplot(fext, aes(filext, N)) +
  geom_point() +
  scale_x_discrete(limits = rev(fext$filext)) +
  scale_y_log10() +
  coord_flip() + 
  xlab("Common file extensions\n(>10 occurences) ") +
  ylab("Number of files")
```

## Look for a word

Let's find summaries containing word "CRISPR".

```{r}
crispr <- suppfilenames %>% 
  filter(str_detect(str_to_lower(summary), "crispr"))
crispr
```


We have `r nrow(crispr)` GEO series containing word "crispr" in summary.

When people started to publish experiments using crispr?

```{r}
crispr %>% 
  ggplot(aes(lubridate::ymd(PDAT))) +
  geom_histogram(aes(y = cumsum(..count..)), bins = 30) +
  labs(title = "Number of Entrez GEO series mentioning CRISPR in summary",
       caption = "Data: Entrez GEO",
       y = "Cumulative number of studies",
       x = "Publication date")
```


## Replace parts of a string

`str_replace()` 

Commonly strings are removed by replacing them with an empty string -- `str_remove()`

Let's suppose we want to fix these ftp links, as these links have string prepended before URL. 
We want to remove this "SRASRP.." part to get bare URL. 
We do this by replacing "SRASRP.." with empty string "".
(alternatively you can extract URL)

```{r}
set.seed(2)
ftplinks <- suppfilenames %>% 
  select(ExtRelations) %>% 
  sample_n(5)
ftplinks
```

```{r}
ftplinks$ExtRelations[2] %>% str_replace("SRASRP[0-9]+", "")
```


## String split

Let's split
```{r}
str_split("A\nB", "\n")
```

Split summaries by word boundaries/whitespace.
```{r}
sums <- suppfilenames %>%
  # sample_n(10) %>% 
  select(summary, Accession)
sums
```

Smaller scale. Go to `?regex` and compare different regexes for splitting.

> The symbol \w matches a ‘word’ character (a synonym for [[:alnum:]_], an extension) and \W is its negation ([^[:alnum:]_]). Symbols \d, \s, \D and \S denote the digit and space classes and their negations (these are all extensions).

```{r}
summary100 <- sums$summary[100] %>% str_split("\\s+")
summary100 <- summary100 %>% unlist
```

Use of str_split within dplyr
```{r}
sums %>%
  sample_n(10) %>% 
  mutate(words = str_split(summary, "\\s")) %>% 
  select(-summary) %>% 
  unnest %>% 
  count(words) %>% 
  arrange(desc(n))
```

- `str_to_lower()`

```{r}
str_to_lower(summary100)
```


- str_to_title()
```{r}
suppfilenames$title[1:10]
str_to_title(suppfilenames$title[1:10])
```


- `str_to_upper()`

```{r}
suppfilenames$title[1:10]
str_to_upper(suppfilenames$title[1:10])
```

- `str_trunc()`

```{r}
str_trunc(suppfilenames$title[1:10], width = 30, side = "right")
str_trunc(suppfilenames$title[1:10], width = 30, side = "center")
str_trunc(suppfilenames$title[1:10], width = 30, side = "left")
```

- `str_wrap()`

```{r}
str_wrap(suppfilenames$title[1], width = 30)
```


## Generate strings from data

- `paste()`

```{r}
# letters
paste("one", letters)
paste("one", letters, collapse = " + ")
paste("one", letters, sep = "+")
```

- `paste0()`

```{r}
paste0("one", letters)
```

- `str_c()` (analogue of paste0())

```{r}
str_c("XXX", summary100)
str_c(summary100, collapse = " ")
```

- `sprintf()`

```{r}
todays_date <- Sys.Date()
todays_date
todays_temp <- 7
sprintf("Today is %s and temperature is %s", todays_date, 7)
```

- `glue()`

```{r}
library(glue)
?glue
glue("Answer to 1 + 1 is {1 + 1}.")
```



## Tidy text analysis

### Tidy data has a specific structure:

- Each variable is a column
- Each observation is a row
- Each type of observational unit is a table

### Tidy text format

- Tidy text format as being a table with one-token-per-row. 
- A token is a meaningful unit of text, such as a word,
- tokenizing is splitting text to meaningful units

Book: http://tidytextmining.com

Unnest tokens (words)

```{r}
# install.packages("tidytext")
library(tidytext)
tidy_sums <- sums %>% 
  unnest_tokens(word, summary)
tidy_sums
```

Now that the data is in one-word-per-row format..

We will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. 
We can remove stop words (kept in the tidytext dataset stop_words) with an `anti_join()`.
```{r}
data(stop_words)
tidy_sums <- tidy_sums %>%
  anti_join(stop_words)
tidy_sums
```

Now we can count words.
```{r}
tidy_sums %>%
  count(word, sort = TRUE)
```

Words with more than 2000 occurences:
```{r}
tidy_sums %>%
  count(word, sort = TRUE) %>%
  filter(n > 2000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


### Sentiment analysis

What is the tone of research abstracts?
```{r}
library(textdata)
get_sentiments()

# Different sentiment databases
get_sentiments("nrc")$sentiment %>% unique
get_sentiments("afinn")$score %>% unique
get_sentiments("bing")$sentiment %>% unique()
```

Abstract scores 1
```{r}
tidy_sums %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(Accession) %>% 
  summarise(sentiment_value = sum(value)) %>% 
  ggplot(aes(sentiment_value)) +
  geom_histogram(bins = 40)
```

Let's see most negative and positive abstracts:
```{r}
get_sentiments(lexicon = "bing")$sentiment %>% unique()
summary_sentiments <- tidy_sums %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(Accession, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment = positive - negative)
summary_sentiments
```
Sligtly more negative sentiment in abstracts
```{r}
summary_sentiments %>% 
  ggplot(aes(sentiment)) +
  geom_histogram(bins = 40)
```

Most negative summary:
```{r}
summary_sentiments %>% 
  arrange(sentiment) %>%
  left_join(select(suppfilenames, Accession, summary))
```
Let's see what this GSE85013 is..
